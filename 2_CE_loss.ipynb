{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EI80vHxcRo4u"
      },
      "source": [
        "**Доплитература по ArcFace Loss:**\n",
        "\n",
        "Оригинальная статья: https://arxiv.org/pdf/1801.07698.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhF-n904jYJz"
      },
      "source": [
        "# План заданий"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqmhb66-jhaK"
      },
      "source": [
        "Итак, вот, что от вас требуется в этом задании:\n",
        "\n",
        "* Выбрать модель (или несколько моделей) для обучения. Можно брать предобученные на ImageNet, но нельзя использовать модели, предобученные на задачу распознавания лиц.\n",
        "* Обучить эту модель (модели) на CE loss. Добиться accuracy > 0.7.\n",
        "* Реализовать ArcFace loss.\n",
        "* Обучить модель (модели) на ArcFace loss. Добиться accuracy > 0.7.\n",
        "* Написать небольшой отчет по обучению, сравнить CE loss и ArcFace loss.\n",
        "\n",
        "**P.S. Не забывайте сохранять модели после обучения**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzDWCmwN-NU2"
      },
      "source": [
        "#Модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "m39hgTZWrHvt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "import math\n",
        "import pandas as pd\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "import cv2\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import pyplot as plt\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwiSBJa8EZRV",
        "outputId": "6220c326-00ee-4265-b33e-465003f7bd6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'total_images': 20000, 'unique_people': 956, 'images_per_person': {'count': 956.0, 'mean': 20.92050209205021, 'std': 1.5626391151459447, 'min': 3.0, '25%': 20.0, '50%': 21.0, '75%': 22.0, 'max': 27.0}}\n"
          ]
        }
      ],
      "source": [
        "# Код для сортировки изображений, чтобы использовались только те ID, что есть в датасете\n",
        "idf_path = \"D:\\\\dlsPart2\\\\identity_CelebA3.txt\"\n",
        "img_folder = \"D:\\\\dlsPart2\\\\Aligned_Images3\"\n",
        "\n",
        "identity_df = pd.read_csv(idf_path, sep=' ', header=None, names=['filename', 'person_id'])\n",
        "\n",
        "files = set(os.listdir(img_folder))\n",
        "\n",
        "filtered_df = identity_df[identity_df['filename'].isin(files)]\n",
        "\n",
        "num_people = filtered_df['person_id'].nunique()\n",
        "\n",
        "stats = {\n",
        "    'total_images': len(filtered_df),\n",
        "    'unique_people': num_people,\n",
        "    'images_per_person': filtered_df['person_id'].value_counts().describe().to_dict()\n",
        "}\n",
        "print(stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "04ZjrBsQJ_iV"
      },
      "outputs": [],
      "source": [
        "class CelebDataset(Dataset):\n",
        "    def __init__(self, img_folder, identity_data, id_to_label, training=True):\n",
        "        \n",
        "        self.img_folder = img_folder\n",
        "        self.identity_df = identity_data\n",
        "        self.training = training\n",
        "\n",
        "        # Фильтрация (оставляем на всякий случай)\n",
        "        files = set(os.listdir(img_folder))\n",
        "        self.identity_df = self.identity_df[self.identity_df['filename'].isin(files)]\n",
        "\n",
        "        self.id_to_label = id_to_label\n",
        "\n",
        "        self.img_paths = self.identity_df['filename'].tolist()\n",
        "        # Применяем единый словарь для получения меток\n",
        "        self.labels = self.identity_df['person_id'].map(self.id_to_label).tolist()\n",
        "\n",
        "        self.train_transforms = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),  # Отражение по горизонтали\n",
        "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),  # Сдвиги и масштабирование\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Цветовые изменения\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Нормализация для ImageNet\n",
        "        ])\n",
        "\n",
        "        self.val_transforms = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_folder, self.img_paths[idx])\n",
        "        image = cv2.imread(img_path)\n",
        "        if image is None:\n",
        "            raise FileNotFoundError(f\"Ошибка загрузки: {img_path}\")\n",
        "\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = self.train_transforms(image) if self.training else self.val_transforms(image)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSfAH5ZrLeYF",
        "outputId": "60858d52-2271-4eb1-d97c-4d11d75520e9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\odron\\myenv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\odron\\myenv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "num_classes = num_people\n",
        "model = models.resnet50(pretrained=True) # Возьмем resnet50, тк в статье была именно она\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in model.layer4.parameters():  # Размораживаем только верхний блок\n",
        "    param.requires_grad = True\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Dropout(p=0.60), # Типичное значение во время обучения 0.5, в конце повысил, чтобы приподнять val_acc\n",
        "    nn.Linear(model.fc.in_features, num_classes)  # Полносвязный слой для классификации\n",
        ")\n",
        "model = model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nvUgeWM0Mg1-"
      },
      "outputs": [],
      "source": [
        "def train(model, train_dataset, val_dataset, epochs=50, batch_size=32, lr=0.001,\n",
        "          device='cuda', savepath=\"model.pth\", pre_train=None):\n",
        "\n",
        "    if pre_train is not None:\n",
        "        model.load_state_dict(torch.load(pre_train))\n",
        "        print(f\"Загружены предобученные веса из {pre_train}\")\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "    #optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=2e-4) # Основой оптимайзер\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4) # для добивочки :)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.6, patience=3) # Помогал, но часто приходилось руками делать\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model = model.to(device)\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_acc': [],\n",
        "        'best_val_loss': float('inf'),\n",
        "        'best_val_acc': 0.0\n",
        "    }\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
        "\n",
        "        for images, labels in progress_bar:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': loss.item(),\n",
        "                'acc': f\"{100 * correct / total:.2f}%\",\n",
        "                'lr': optimizer.param_groups[0]['lr']\n",
        "            })\n",
        "\n",
        "        epoch_train_loss = running_loss / len(train_dataset)\n",
        "        epoch_train_acc = 100 * correct / total\n",
        "        history['train_loss'].append(epoch_train_loss)\n",
        "        history['train_acc'].append(epoch_train_acc)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Применим TTA, чтобы дообучить модель до 70%, а то она зависла на 69...\n",
        "                # Идея в том, чтобы для каждой картинки в валидационном сете делать предсказание не только для оригинала, но и для его аугментированной версии, а потом усреднять.\n",
        "                outputs_original = model(images)\n",
        "                flipped_images = torch.flip(images, [3])\n",
        "                outputs_flipped = model(flipped_images)\n",
        "                outputs = (torch.softmax(outputs_original, dim=1) + torch.softmax(outputs_flipped, dim=1)) / 2\n",
        "                loss = criterion(torch.log(outputs), labels) \n",
        "\n",
        "                val_loss += loss.item() * images.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_val_loss = val_loss / len(val_dataset)\n",
        "        epoch_val_acc = 100 * val_correct / val_total\n",
        "        history['val_loss'].append(epoch_val_loss)\n",
        "        history['val_acc'].append(epoch_val_acc)\n",
        "\n",
        "        scheduler.step(epoch_val_loss)\n",
        "\n",
        "        if epoch_val_acc > history['best_val_acc']:\n",
        "            history['best_val_acc'] = epoch_val_acc\n",
        "            history['best_val_loss'] = epoch_val_loss\n",
        "            torch.save(model.state_dict(), savepath)\n",
        "            print(f\"Модель сохранена с val_acc: {epoch_val_acc:.2f}%\")\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
        "              f\"Train Loss: {epoch_train_loss:.4f} | Val Loss: {epoch_val_loss:.4f} | \"\n",
        "              f\"Train Acc: {epoch_train_acc:.2f}% | Val Acc: {epoch_val_acc:.2f}%\")\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHd_rsDwAvPg",
        "outputId": "7fcfba8a-1296-4bc0-f435-701d8073d53c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Фактическое количество классов для обучения: 956\n",
            "Number of unique classes in train: 956\n",
            "Number of unique classes in val: 956\n",
            "Classes in val but not in train: 0\n",
            "Classes in train but not in val: 0\n",
            "\n",
            "Train dataset class distribution:\n",
            "count    956.000000\n",
            "mean      16.375523\n",
            "std        1.177533\n",
            "min        2.000000\n",
            "25%       16.000000\n",
            "50%       16.000000\n",
            "75%       17.000000\n",
            "max       21.000000\n",
            "Name: count, dtype: float64\n",
            "\n",
            "Validation dataset class distribution:\n",
            "count    956.000000\n",
            "mean       4.544979\n",
            "std        0.522846\n",
            "min        1.000000\n",
            "25%        4.000000\n",
            "50%        5.000000\n",
            "75%        5.000000\n",
            "max        6.000000\n",
            "Name: count, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Функция для разделения данных с сохранением классов\n",
        "# Сделана, тк изначально сети было сложно обучаться, тк картинки человека могли быть в треине, но их не было в вал выборке\n",
        "def split_by_person(df, test_size=0.2, random_state=42):\n",
        "    train_dfs = []\n",
        "    val_dfs = []\n",
        "\n",
        "    for person_id in df['person_id'].unique():\n",
        "        person_data = df[df['person_id'] == person_id]\n",
        "        if len(person_data) < 2:  # Пропустить классы с 1 изображением\n",
        "            continue\n",
        "        # Разделение изображений одного человека\n",
        "        train_person, val_person = train_test_split(\n",
        "            person_data, test_size=test_size, random_state=42\n",
        "        )\n",
        "        train_dfs.append(train_person)\n",
        "        val_dfs.append(val_person)\n",
        "\n",
        "    train_df = pd.concat(train_dfs, ignore_index=True)\n",
        "    val_df = pd.concat(val_dfs, ignore_index=True)\n",
        "\n",
        "    return train_df, val_df\n",
        "\n",
        "train_df, val_df = split_by_person(filtered_df, test_size=0.20, random_state=42)\n",
        "\n",
        "# Создаем единый словарь для всех данных, которые будут использоваться\n",
        "all_used_ids = pd.concat([train_df, val_df])['person_id'].unique()\n",
        "id_to_label_map = {id_val: i for i, id_val in enumerate(all_used_ids)}\n",
        "\n",
        "num_classes_actual = len(id_to_label_map) \n",
        "print(f\"Фактическое количество классов для обучения: {num_classes_actual}\")\n",
        "\n",
        "# Проверка, что классы совпадают\n",
        "train_classes = set(train_df['person_id'].unique())\n",
        "val_classes = set(val_df['person_id'].unique())\n",
        "print(f\"Number of unique classes in train: {len(train_classes)}\")\n",
        "print(f\"Number of unique classes in val: {len(val_classes)}\")\n",
        "print(f\"Classes in val but not in train: {len(val_classes - train_classes)}\")\n",
        "print(f\"Classes in train but not in val: {len(train_classes - val_classes)}\")\n",
        "\n",
        "#Проверка баланса классов\n",
        "print(\"\\nTrain dataset class distribution:\")\n",
        "print(train_df['person_id'].value_counts().describe())\n",
        "print(\"\\nValidation dataset class distribution:\")\n",
        "print(val_df['person_id'].value_counts().describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "idf_path = \"D:\\\\dlsPart2\\\\identity_CelebA3.txt\"\n",
        "img_folder = \"D:\\\\dlsPart2\\\\Aligned_Images3\"\n",
        "\n",
        "train_dataset = CelebDataset(img_folder, train_df, id_to_label_map, training=True)\n",
        "val_dataset = CelebDataset(img_folder, val_df, id_to_label_map, training=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zoIM6D0WSjN",
        "outputId": "1c3112f4-5d99-4c45-a545-43a9150ad8fd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\odron\\AppData\\Local\\Temp\\ipykernel_24928\\3096948869.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(pre_train))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Загружены предобученные веса из best_resnet50VBEST.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10: 100%|██████████| 245/245 [02:49<00:00,  1.45it/s, loss=0.0574, acc=99.23%, lr=1e-6]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Модель сохранена с val_acc: 71.62%\n",
            "Epoch 1/10 | Train Loss: 0.0629 | Val Loss: 1.3902 | Train Acc: 99.23% | Val Acc: 71.62%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10: 100%|██████████| 245/245 [01:57<00:00,  2.08it/s, loss=0.0899, acc=99.37%, lr=1e-6]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Модель сохранена с val_acc: 71.76%\n",
            "Epoch 2/10 | Train Loss: 0.0583 | Val Loss: 1.3805 | Train Acc: 99.37% | Val Acc: 71.76%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10: 100%|██████████| 245/245 [01:59<00:00,  2.05it/s, loss=0.0317, acc=99.26%, lr=1e-6]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 | Train Loss: 0.0619 | Val Loss: 1.3937 | Train Acc: 99.26% | Val Acc: 71.42%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10: 100%|██████████| 245/245 [01:59<00:00,  2.06it/s, loss=0.0243, acc=99.29%, lr=1e-6]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 | Train Loss: 0.0611 | Val Loss: 1.3895 | Train Acc: 99.29% | Val Acc: 71.48%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10: 100%|██████████| 245/245 [01:58<00:00,  2.06it/s, loss=0.0745, acc=99.34%, lr=1e-6]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 | Train Loss: 0.0607 | Val Loss: 1.3853 | Train Acc: 99.34% | Val Acc: 71.65%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10: 100%|██████████| 245/245 [02:01<00:00,  2.01it/s, loss=0.0756, acc=99.17%, lr=1e-6]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 | Train Loss: 0.0619 | Val Loss: 1.3899 | Train Acc: 99.17% | Val Acc: 71.74%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/10: 100%|██████████| 245/245 [02:05<00:00,  1.95it/s, loss=0.0723, acc=99.30%, lr=6e-7]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 | Train Loss: 0.0611 | Val Loss: 1.3823 | Train Acc: 99.30% | Val Acc: 71.60%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/10: 100%|██████████| 245/245 [02:01<00:00,  2.01it/s, loss=0.039, acc=99.28%, lr=6e-7] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 | Train Loss: 0.0607 | Val Loss: 1.3919 | Train Acc: 99.28% | Val Acc: 71.53%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/10: 100%|██████████| 245/245 [02:02<00:00,  2.00it/s, loss=0.106, acc=99.25%, lr=6e-7] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 | Train Loss: 0.0621 | Val Loss: 1.3934 | Train Acc: 99.25% | Val Acc: 71.58%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/10: 100%|██████████| 245/245 [02:28<00:00,  1.65it/s, loss=0.0508, acc=99.27%, lr=6e-7]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 | Train Loss: 0.0596 | Val Loss: 1.3864 | Train Acc: 99.27% | Val Acc: 71.74%\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAMG1JREFUeJzt3Xt8U3We//F3krZpgV5AbLlYbg4MiFiQQhfY8bJWK2of4mXsIiMXceahAwh2dKUKLYhSFWHqcl0YL+PuIKijjiuIMnUQVEZudlZ/goyCtjK2gKMNFOwlOb8/0qZNm5amtP229PV8PPJIzjff7zmf5Jz2vHNyktgsy7IEAABgiN10AQAAoGMjjAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwKsR0AY3h8Xj0j3/8Q5GRkbLZbKbLAQAAjWBZlk6cOKFevXrJbq//+Ee7CCP/+Mc/FB8fb7oMAADQBAUFBbrgggvqvb9dhJHIyEhJ3gcTFRVluBoAANAYLpdL8fHxvv14fdpFGKl6ayYqKoowAgBAO3OmUyw4gRUAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgVNBhZPv27UpNTVWvXr1ks9n0+uuvN3rsBx98oJCQEA0fPjzYxQIAgHNU0GGkpKRECQkJWrlyZVDjfvjhB02ePFlXXXVVsIsEAADnsKC/Dn78+PEaP3580Au6++67dfvtt8vhcAR1NAUAAJzbWuWckeeee06HDh1SVlZWo/qXlpbK5XL5XQAAwLmpxX8o7+9//7vmzp2rHTt2KCSkcYvLzs7WwoULW7gytDrLkspPSad/kH78IfB12UnJZpccoZIjTLKHVt6umg7xXjtC/W87Qiv7hkmOkLpjA91nd0hn+PEmAM3I4/H+jZeVSO5SyfJ4/y9YVuXt+i713K/a7cHOp77+jZhP1bJtdikkXAqNqHUdLoVEeK9DO9Xt4wjj/08NLRpG3G63br/9di1cuFCDBg1q9LiMjAylp6f7pqt+grjZbc2U/nlICutSeeksObvUmo70XteeDu3UMTcky/L+I6kvTJzp2lNuoup62GoElXoCT+37AwaeAOEn4H21btecl+92aP2By+92qGQ/Bz4M5/F4twl3meQur7yUVU/73VfVXuHfp+q2J1B7jXk6Qr07gdBOlTuFiMrpqrbw6vt8O49O1TuQc+H5DoZlSRU/ev/eS094r8tOVoeJ0srrshO1pmv3qTFdfsr0o2pDbAECTETgtvqufdtxeIDrAGPa8D6rRcPIiRMntGfPHn388ceaOXOmJMnj8ciyLIWEhOidd97Rv/3bv9UZ53Q65XQ6W7I0r6/el47sbeJgmzegOLvUCCvBTEfWCD+V044WP1DlZVnefw5NCRM/Fp99oLA5pIgYKTym7rUz0vtqw29HVLmT8ZTXul1jR+PbEZXXGFvuPx/LU/uJqN55taWM1Fg2e+NCz9mGIctda+ceTHAoDzzWU2O9tRe+HUCtoBJaO9RE1Lq/dtCpGYRqzSMkoun/B9wV1cEgUBAoO1k3MNQbICrbLHfzPodVbI7KbdHh3Y5ttsrr+i617leg/sHMo76+QcyjqgbLLZX/KFWc9r8uP123reJ0jf9DlUeKy09Jp1vmaa6j3uBSeX3No1LskFYqplZpLTnzqKgoffLJJ35tq1at0rvvvqtXXnlF/fv3b8nFn9llD0iuI2f+A609LUmyKv/wTzRfPQ5nw2Gl3uku3j/sUlfDQeL0997bPxaf/U7AHhI4TDTmOqyLmYTucdcNKk0OOfXdFyA81Wxv9O1aywsUAC2P95Wrfmz1p7Ll2CqDUNWRqZq3QwO01TwiVXtcraNUnnLvDsJ3OeV9/spP1Wo/Xd3mLq0ureJH7+X09y37FDjC6j9qExLu3e6qAkfNoxI1a21uoZ0D//8JdDQ50Iuw2v1CnG36VXqLsSzv33Od4FK1LZ6u57qq7+kG+tToW3N+NQNlRWVAUj3b8BUPtcrTEEjQYeTkyZP64osvfNOHDx9WXl6eunXrpj59+igjI0NHjhzRCy+8ILvdrosvvthvfGxsrMLDw+u0G/HT4D8VJI/Hu6IDhpXahytrTjdweNNd5p23u1Q6VSqd+q55H2d97KFNCxPhMd5/Lu3tn4nd4b0o3HQlwbOsyvBTMwjVuh1U6Ck/8/yqbtsdTQsAZ+wfqjqhw+4w/Uz787ir/6mXn6r+R+8XZhq6r4GgU1Grf5Wqo0qlxU2r2REW4EhsA0dr/YJCgH6hnTveW1QtxWaTQsK8l/Do1lmmu7zhAFPzCE43cwcIgg4je/bs0ZVXXumbrjq3Y8qUKXr++ef17bffKj8/v/kqbGvsdu8fqbOLpLjmmWdFWcPvs/q9X1vPtLtMckYFFyo66nkv7ZHNVr0DVyfT1XQcdkf1zrwlWZZUUeofXipOBw469pB6jkhUBomQsJatFe2L7/9GlOlKGmSzLMsyXcSZuFwuRUdHq7i4WFFRbfsJBQAAXo3df3PsDQAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUUGHke3btys1NVW9evWSzWbT66+/3mD/V199VVdffbXOP/98RUVFacyYMXr77bebWi8AADjHBB1GSkpKlJCQoJUrVzaq//bt23X11Vdr8+bN2rt3r6688kqlpqbq448/DrpYAABw7rFZlmU1ebDNptdee00TJkwIatzQoUOVlpamzMzMRvV3uVyKjo5WcXGxoqKimlApAABobY3df4e0Yk2SJI/HoxMnTqhbt2719iktLVVpaalv2uVytUZpAADAgFY/gfWpp57SyZMnddttt9XbJzs7W9HR0b5LfHx8K1YIAABaU6uGkfXr12vhwoV66aWXFBsbW2+/jIwMFRcX+y4FBQWtWCUAAGhNrfY2zYYNG3TXXXfp5ZdfVnJycoN9nU6nnE5nK1UGAABMapUjIy+++KKmTZumF198Uddff31rLBIAALQTQR8ZOXnypL744gvf9OHDh5WXl6du3bqpT58+ysjI0JEjR/TCCy9I8r41M2XKFD399NNKSkpSYWGhJCkiIkLR0dHN9DAAAEB7FfSRkT179mjEiBEaMWKEJCk9PV0jRozwfUz322+/VX5+vq//2rVrVVFRoRkzZqhnz56+y+zZs5vpIQAAgPbsrL5npLXwPSMAALQ/jd1/89s0AADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMCjqMbN++XampqerVq5dsNptef/31M47Ztm2bLr30UjmdTv3kJz/R888/34RSAQDAuSjoMFJSUqKEhAStXLmyUf0PHz6s66+/XldeeaXy8vI0Z84c3XXXXXr77beDLhYAAJx7QoIdMH78eI0fP77R/desWaP+/ftr6dKlkqQhQ4bo/fff129/+1ulpKQEu3gAAHCOafFzRnbu3Knk5GS/tpSUFO3cubOlFw0AANqBoI+MBKuwsFBxcXF+bXFxcXK5XDp9+rQiIiLqjCktLVVpaalv2uVytXSZAADAkDb5aZrs7GxFR0f7LvHx8aZLAgAALaTFw0iPHj1UVFTk11ZUVKSoqKiAR0UkKSMjQ8XFxb5LQUFBS5cJAAAMafG3acaMGaPNmzf7tW3dulVjxoypd4zT6ZTT6Wzp0gAAQBsQ9JGRkydPKi8vT3l5eZK8H93Ny8tTfn6+JO9RjcmTJ/v633333Tp06JD+4z/+QwcOHNCqVav00ksv6b777mueRwAAANq1oMPInj17NGLECI0YMUKSlJ6erhEjRigzM1OS9O233/qCiST1799fmzZt0tatW5WQkKClS5fqd7/7HR/rBQAAkiSbZVmW6SLOxOVyKTo6WsXFxYqKijJdDgAAaITG7r/b5KdpAABAx0EYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARoWYLgAA0LF4PB6VlZWZLgPNIDQ0VA6H46znQxgBALSasrIyHT58WB6Px3QpaCYxMTHq0aOHbDZbk+fRpDCycuVKLVmyRIWFhUpISNDy5cs1evToevvn5ORo9erVys/PV/fu3XXrrbcqOztb4eHhTS4cANC+WJalb7/9Vg6HQ/Hx8bLbOVOgPbMsS6dOndLRo0clST179mzyvIIOIxs3blR6errWrFmjpKQk5eTkKCUlRZ9//rliY2Pr9F+/fr3mzp2rZ599VmPHjtXBgwc1depU2Ww2LVu2rMmFAwDal4qKCp06dUq9evVSp06dTJeDZhARESFJOnr0qGJjY5v8lk3QsXTZsmX65S9/qWnTpumiiy7SmjVr1KlTJz377LMB+3/44YcaN26cbr/9dvXr10/XXHONJk6cqF27djWpYABA++R2uyVJYWFhhitBc6oKluXl5U2eR1BhpKysTHv37lVycnL1DOx2JScna+fOnQHHjB07Vnv37vWFj0OHDmnz5s267rrr6l1OaWmpXC6X3wUAcG44m3ML0PY0x/oM6m2a48ePy+12Ky4uzq89Li5OBw4cCDjm9ttv1/Hjx/Wv//qvsixLFRUVuvvuu/XQQw/Vu5zs7GwtXLgwmNIAAEA71eJnD23btk2LFy/WqlWrtG/fPr366qvatGmTFi1aVO+YjIwMFRcX+y4FBQUtXSYAAK2mX79+ysnJMV1GmxHUkZHu3bvL4XCoqKjIr72oqEg9evQIOGb+/Pm64447dNddd0mShg0bppKSEv3qV7/Sww8/HPBsaqfTKafTGUxpAAA0uzO9BZGVlaUFCxYEPd/du3erc+fOTazK64orrtDw4cPPiVAT1JGRsLAwjRw5Urm5ub42j8ej3NxcjRkzJuCYU6dO1QkcVWfbWpYVbL0AALSab7/91nfJyclRVFSUX9v999/v61t1KkJjnH/++XyiqIag36ZJT0/XunXr9Pvf/1779+/XPffco5KSEk2bNk2SNHnyZGVkZPj6p6amavXq1dqwYYMOHz6srVu3av78+UpNTW2Wb20DAKCl9OjRw3eJjo6WzWbzTR84cECRkZF66623NHLkSDmdTr3//vv68ssvdeONNyouLk5dunTRqFGj9Oc//9lvvrXfprHZbPrd736nm266SZ06ddLAgQP1xhtvnFXtf/zjHzV06FA5nU7169dPS5cu9bt/1apVGjhwoMLDwxUXF6dbb73Vd98rr7yiYcOGKSIiQuedd56Sk5NVUlJyVvU0JOjvGUlLS9OxY8eUmZmpwsJCDR8+XFu2bPGd1Jqfn+93JGTevHmy2WyaN2+ejhw5ovPPP1+pqal67LHHmu9RAADaHcuydLrcbWTZEaGOZvtUz9y5c/XUU09pwIAB6tq1qwoKCnTdddfpsccek9Pp1AsvvKDU1FR9/vnn6tOnT73zWbhwoZ588kktWbJEy5cv16RJk/T111+rW7duQde0d+9e3XbbbVqwYIHS0tL04Ycf6te//rXOO+88TZ06VXv27NG9996r//7v/9bYsWP1z3/+Uzt27JDkPRo0ceJEPfnkk7rpppt04sQJ7dixo0XfzWjSN7DOnDlTM2fODHjftm3b/BcQEqKsrCxlZWU1ZVEAgHPU6XK3Lsp828iyP3skRZ3CmucXUR555BFdffXVvulu3bopISHBN71o0SK99tpreuONN+rdd0rS1KlTNXHiREnS4sWL9Z//+Z/atWuXrr322qBrWrZsma666irNnz9fkjRo0CB99tlnWrJkiaZOnar8/Hx17txZN9xwgyIjI9W3b1+NGDFCkjeMVFRU6Oabb1bfvn0lec/3bEl8Fy8AAGchMTHRb/rkyZO6//77NWTIEMXExKhLly7av3+/8vPzG5zPJZdc4rvduXNnRUVF+b5qPVj79+/XuHHj/NrGjRunv//973K73br66qvVt29fDRgwQHfccYf+8Ic/6NSpU5KkhIQEXXXVVRo2bJh+/vOfa926dfr++++bVEdj8UN5AAAjIkId+uyRFGPLbi61PxVz//33a+vWrXrqqaf0k5/8RBEREbr11lvP+EvFoaGhftM2m63FflAwMjJS+/bt07Zt2/TOO+8oMzNTCxYs0O7duxUTE6OtW7fqww8/1DvvvKPly5fr4Ycf1kcffaT+/fu3SD2EEQCAETabrdneKmlLPvjgA02dOlU33XSTJO+Rkq+++qpVaxgyZIg++OCDOnUNGjTI9+GRkJAQJScnKzk5WVlZWYqJidG7776rm2++WTabTePGjdO4ceOUmZmpvn376rXXXlN6enqL1HvubQUAABg0cOBAvfrqq0pNTZXNZtP8+fNb7AjHsWPHlJeX59fWs2dP/eY3v9GoUaO0aNEipaWlaefOnVqxYoVWrVolSXrzzTd16NAhXXbZZeratas2b94sj8ejn/70p/roo4+Um5ura665RrGxsfroo4907NgxDRkypEUeg0QYAQCgWS1btkx33nmnxo4dq+7du+vBBx9ssd9YW79+vdavX+/XtmjRIs2bN08vvfSSMjMztWjRIvXs2VOPPPKIpk6dKkmKiYnRq6++qgULFujHH3/UwIED9eKLL2ro0KHav3+/tm/frpycHLlcLvXt21dLly7V+PHjW+QxSJLNagffPOZyuRQdHa3i4mJFRUWZLgcA0AQ//vijDh8+rP79+ys8PNx0OWgmDa3Xxu6/+TQNAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgBAC7viiis0Z84c02W0WYQRAADqkZqaqmuvvTbgfTt27JDNZtP//d//nfVynn/+ecXExJz1fNorwggAAPWYPn26tm7dqm+++abOfc8995wSExN1ySWXGKjs3EIYAQCgHjfccIPOP/98Pf/8837tJ0+e1Msvv6zp06fru+++08SJE9W7d2916tRJw4YN04svvtisdeTn5+vGG29Uly5dFBUVpdtuu01FRUW++//2t7/pyiuvVGRkpKKiojRy5Ejt2bNHkvT1118rNTVVXbt2VefOnTV06FBt3ry5Wes7WyGmCwAAdFCWJZWfMrPs0E6SzXbGbiEhIZo8ebKef/55Pfzww7JVjnn55Zfldrs1ceJEnTx5UiNHjtSDDz6oqKgobdq0SXfccYcuvPBCjR49+qxL9Xg8viDy3nvvqaKiQjNmzFBaWpq2bdsmSZo0aZJGjBih1atXy+FwKC8vT6GhoZKkGTNmqKysTNu3b1fnzp312WefqUuXLmddV3MijAAAzCg/JS3uZWbZD/1DCuvcqK533nmnlixZovfee09XXHGFJO9bNLfccouio6MVHR2t+++/39d/1qxZevvtt/XSSy81SxjJzc3VJ598osOHDys+Pl6S9MILL2jo0KHavXu3Ro0apfz8fD3wwAMaPHiwJGngwIG+8fn5+brllls0bNgwSdKAAQPOuqbmxts0AAA0YPDgwRo7dqyeffZZSdIXX3yhHTt2aPr06ZIkt9utRYsWadiwYerWrZu6dOmit99+W/n5+c2y/P379ys+Pt4XRCTpoosuUkxMjPbv3y9JSk9P11133aXk5GQ9/vjj+vLLL3197733Xj366KMaN26csrKymuWE2+bGkREAgBmhnbxHKEwtOwjTp0/XrFmztHLlSj333HO68MILdfnll0uSlixZoqefflo5OTkaNmyYOnfurDlz5qisrKwlKg9owYIFuv3227Vp0ya99dZbysrK0oYNG3TTTTfprrvuUkpKijZt2qR33nlH2dnZWrp0qWbNmtVq9Z0JR0YAAGbYbN63SkxcGnG+SE233Xab7Ha71q9frxdeeEF33nmn7/yRDz74QDfeeKN+8YtfKCEhQQMGDNDBgweb7WkaMmSICgoKVFBQ4Gv77LPP9MMPP+iiiy7ytQ0aNEj33Xef3nnnHd1888167rnnfPfFx8fr7rvv1quvvqrf/OY3WrduXbPV1xw4MgIAwBl06dJFaWlpysjIkMvl0tSpU333DRw4UK+88oo+/PBDde3aVcuWLVNRUZFfUGgMt9utvLw8vzan06nk5GQNGzZMkyZNUk5OjioqKvTrX/9al19+uRITE3X69Gk98MADuvXWW9W/f39988032r17t2655RZJ0pw5czR+/HgNGjRI33//vf7yl79oyJAhZ/uUNCvCCAAAjTB9+nQ988wzuu6669SrV/WJt/PmzdOhQ4eUkpKiTp066Ve/+pUmTJig4uLioOZ/8uRJjRgxwq/twgsv1BdffKE//elPmjVrli677DLZ7XZde+21Wr58uSTJ4XDou+++0+TJk1VUVKTu3bvr5ptv1sKFCyV5Q86MGTP0zTffKCoqStdee61++9vfnuWz0bxslmVZpos4E5fLpejoaBUXFysqKsp0OQCAJvjxxx91+PBh9e/fX+Hh4abLQTNpaL02dv/NOSMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCACgVbWDD3EiCM2xPgkjAIBW4XA4JKlVvyYdLe/UKe8vL1f9SnBT8KVnAIBWERISok6dOunYsWMKDQ2V3c7r4fbMsiydOnVKR48eVUxMjC9sNgVhBADQKmw2m3r27KnDhw/r66+/Nl0OmklMTIx69OhxVvMgjAAAWk1YWJgGDhzIWzXniNDQ0LM6IlKFMAIAaFV2u52vg4cf3rADAABGNSmMrFy5Uv369VN4eLiSkpK0a9euBvv/8MMPmjFjhnr27Cmn06lBgwZp8+bNTSoYAACcW4J+m2bjxo1KT0/XmjVrlJSUpJycHKWkpOjzzz9XbGxsnf5lZWW6+uqrFRsbq1deeUW9e/fW119/rZiYmOaoHwAAtHM2K8hvK0lKStKoUaO0YsUKSZLH41F8fLxmzZqluXPn1um/Zs0aLVmyRAcOHGjyZ5Ab+xPEAACg7Wjs/juot2nKysq0d+9eJScnV8/AbldycrJ27twZcMwbb7yhMWPGaMaMGYqLi9PFF1+sxYsXy+1217uc0tJSuVwuvwsAADg3BRVGjh8/Lrfbrbi4OL/2uLg4FRYWBhxz6NAhvfLKK3K73dq8ebPmz5+vpUuX6tFHH613OdnZ2YqOjvZd4uPjgykTAAC0Iy3+aRqPx6PY2FitXbtWI0eOVFpamh5++GGtWbOm3jEZGRkqLi72XQoKClq6TAAAYEhQJ7B2795dDodDRUVFfu1FRUX1fvtaz54963wpypAhQ1RYWKiysjKFhYXVGeN0OuV0OoMpDQAAtFNBHRkJCwvTyJEjlZub62vzeDzKzc3VmDFjAo4ZN26cvvjiC3k8Hl/bwYMH1bNnz4BBBAAAdCxBv02Tnp6udevW6fe//73279+ve+65RyUlJZo2bZokafLkycrIyPD1v+eee/TPf/5Ts2fP1sGDB7Vp0yYtXrxYM2bMaL5HAQAA2q2gv2ckLS1Nx44dU2ZmpgoLCzV8+HBt2bLFd1Jrfn6+3y8xxsfH6+2339Z9992nSy65RL1799bs2bP14IMPNt+jAAAA7VbQ3zNiAt8zAgBA+9Mi3zMCAADQ3AgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwqklhZOXKlerXr5/Cw8OVlJSkXbt2NWrchg0bZLPZNGHChKYsFgAAnIOCDiMbN25Uenq6srKytG/fPiUkJCglJUVHjx5tcNxXX32l+++/Xz/72c+aXCwAADj3BB1Gli1bpl/+8peaNm2aLrroIq1Zs0adOnXSs88+W+8Yt9utSZMmaeHChRowYMBZFQwAAM4tQYWRsrIy7d27V8nJydUzsNuVnJysnTt31jvukUceUWxsrKZPn970SgEAwDkpJJjOx48fl9vtVlxcnF97XFycDhw4EHDM+++/r2eeeUZ5eXmNXk5paalKS0t90y6XK5gyAQBAO9Kin6Y5ceKE7rjjDq1bt07du3dv9Ljs7GxFR0f7LvHx8S1YJQAAMCmoIyPdu3eXw+FQUVGRX3tRUZF69OhRp/+XX36pr776Sqmpqb42j8fjXXBIiD7//HNdeOGFdcZlZGQoPT3dN+1yuQgkAACco4IKI2FhYRo5cqRyc3N9H8/1eDzKzc3VzJkz6/QfPHiwPvnkE7+2efPm6cSJE3r66afrDRhOp1NOpzOY0gAAQDsVVBiRpPT0dE2ZMkWJiYkaPXq0cnJyVFJSomnTpkmSJk+erN69eys7O1vh4eG6+OKL/cbHxMRIUp12AADQMQUdRtLS0nTs2DFlZmaqsLBQw4cP15YtW3wntebn58tu54tdAQBA49gsy7JMF3EmLpdL0dHRKi4uVlRUlOlyAABAIzR2/80hDAAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUU0KIytXrlS/fv0UHh6upKQk7dq1q96+69at089+9jN17dpVXbt2VXJycoP9AQBAxxJ0GNm4caPS09OVlZWlffv2KSEhQSkpKTp69GjA/tu2bdPEiRP1l7/8RTt37lR8fLyuueYaHTly5KyLBwAA7Z/NsiwrmAFJSUkaNWqUVqxYIUnyeDyKj4/XrFmzNHfu3DOOd7vd6tq1q1asWKHJkyc3apkul0vR0dEqLi5WVFRUMOUCAABDGrv/DurISFlZmfbu3avk5OTqGdjtSk5O1s6dOxs1j1OnTqm8vFzdunWrt09paalcLpffBQAAnJuCCiPHjx+X2+1WXFycX3tcXJwKCwsbNY8HH3xQvXr18gs0tWVnZys6Otp3iY+PD6ZMAADQjrTqp2kef/xxbdiwQa+99prCw8Pr7ZeRkaHi4mLfpaCgoBWrBAAArSkkmM7du3eXw+FQUVGRX3tRUZF69OjR4NinnnpKjz/+uP785z/rkksuabCv0+mU0+kMpjQAANBOBXVkJCwsTCNHjlRubq6vzePxKDc3V2PGjKl33JNPPqlFixZpy5YtSkxMbHq1AADgnBPUkRFJSk9P15QpU5SYmKjRo0crJydHJSUlmjZtmiRp8uTJ6t27t7KzsyVJTzzxhDIzM7V+/Xr169fPd25Jly5d1KVLl2Z8KAAAoD0KOoykpaXp2LFjyszMVGFhoYYPH64tW7b4TmrNz8+X3V59wGX16tUqKyvTrbfe6jefrKwsLViw4OyqBwAA7V7Q3zNiAt8zAgBA+9Mi3zMCAADQ3AgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAqBDTBZi04I3/py+PnVSow65Qh02hDrvCHHbvdEit6cq2qumQ2v0dNoWG1Jp22BUWUj1dPe/K++122e02009Do1iWpQqPpQq3pQqPRxVuS+Uej9yVbeVu7+3yqvtr9a2+ru5bNV3h8ajcbcldOU6SbLLJZpNsUuW1d1qSbDabr91uq+6nGu2Bxss3XaNfA/OuMw/ffXXHB5y3t6rqeVc9mQ3cZ7PV1149Ve8Yv77+j0sB7vObDjCmaqglyWNZsqzqa8uSLFny+LVZ3r4eb7ul6r6eqvssy9vPkreP5e2rWvPyWN7twFOjr8e7UL95eSxJlddV8/I2WX5jvSPq2S78noea669y+iy3rzrPawPzkd90/dtZfes94HZV3zqvOY9Gbm9N3dYCbWdV20fNde63Hq3qbaJ63VdP+7ZBT4Cxqjnt7VNz+6u93XpqLb9mPbW32drzkKQQu012u00OW+V15W1H5W27b9r7PyvEYZPdVt3Pd7+jepzvfrvksNsr+8lvTIhfP//b1f3km6et9kpqQzp0GMkr+EF5BT8YrSHEbqsOKyE1gk+AMFPnvhohKcRu99vhV7g9KvdYctcKB77QUNnHXRkOaoYHX6ioavdYcleGBABA+2S3yT+01Aw1dpuemZKoSy6IMVJbhw4jv7lmkL47WaYyt0flbo/KK7yv0H3T7srpilrTvr51+1fUnK7w7ujLaoytvVP37uzdOl1u6Ek4CzabFGq3y2G3+Y4UOew2hdq9CT/U7j2C5LB7A1SI3RuaQhw2hTjsldPesVXtjsrkXvmC1/uKtsar3er2mtP+r9ar7/fOqW7/6mmp1rha81Ct6epXZN6BVsDx1fOu4nssvroq51GzjhqNAe+rbz41FhWof+WzUGN87XF166o5H3vNI0I27z+0um022W3V0zZVtdn8jirZ7Q2MrToqYKtnrE0B2qrnUdVur7F8vyMbvsdWvV3VfLyBti2/9d/E7avmemvMNlZ7PrW3s0DrvinbXK3VH/C+mttPoO2tof7VNdQe551XzfVVtb6l6m3E3sC2ZA+wzdWcrtqefNtbgO2i5vZkDzCPmtOBl119lMFjeV+0eSpfuLmtytuW90ih90Wet5/bY/mu3XX6W95+lWM8lnx9ao6teoHo8Y1V5diGXzR6LMnjtmqsbX8mX3R26DDys4Hnt/oyq45EVIWTcrenRtipEV4qak3XCDhlbo/3yEeN4FPhtrxBoM6O3l7runLHXztAVLY77LXaAs6rui8AoG2oevvIL/BUBp0KT83A4x9gqi79uncyVnuHDiMmeN/Lcyg81GG6FADAOcRms8lR+VZMe8OnaQAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGNSmMrFy5Uv369VN4eLiSkpK0a9euBvu//PLLGjx4sMLDwzVs2DBt3ry5ScUCAIBzT9BhZOPGjUpPT1dWVpb27dunhIQEpaSk6OjRowH7f/jhh5o4caKmT5+ujz/+WBMmTNCECRP06aefnnXxAACg/bNZtX9d6QySkpI0atQorVixQpLk8XgUHx+vWbNmae7cuXX6p6WlqaSkRG+++aav7V/+5V80fPhwrVmzplHLdLlcio6OVnFxsaKiooIpFwAAGNLY/XdQR0bKysq0d+9eJScnV8/AbldycrJ27twZcMzOnTv9+ktSSkpKvf0lqbS0VC6Xy+8CAADOTUGFkePHj8vtdisuLs6vPS4uToWFhQHHFBYWBtVfkrKzsxUdHe27xMfHB1MmAABoR9rkr/ZmZGQoPT3dN11cXKw+ffpwhAQAgHakar99pjNCggoj3bt3l8PhUFFRkV97UVGRevToEXBMjx49guovSU6nU06n0zdd9WA4QgIAQPtz4sQJRUdH13t/UGEkLCxMI0eOVG5uriZMmCDJewJrbm6uZs6cGXDMmDFjlJubqzlz5vjatm7dqjFjxjR6ub169VJBQYEiIyNls9mCKblBLpdL8fHxKigo4MTYNoD10fawTtoW1kfbwvo4M8uydOLECfXq1avBfkG/TZOenq4pU6YoMTFRo0ePVk5OjkpKSjRt2jRJ0uTJk9W7d29lZ2dLkmbPnq3LL79cS5cu1fXXX68NGzZoz549Wrt2baOXabfbdcEFFwRbaqNFRUWxIbUhrI+2h3XStrA+2hbWR8MaOiJSJegwkpaWpmPHjikzM1OFhYUaPny4tmzZ4jtJNT8/X3Z79XmxY8eO1fr16zVv3jw99NBDGjhwoF5//XVdfPHFwS4aAACcg4L+npFzCd9f0rawPtoe1knbwvpoW1gfzadD/zaN0+lUVlaW38myMIf10fawTtoW1kfbwvpoPh36yAgAADCvQx8ZAQAA5hFGAACAUYQRAABgFGEEAAAY1aHDyMqVK9WvXz+Fh4crKSlJu3btMl1Sh5Sdna1Ro0YpMjJSsbGxmjBhgj7//HPTZaHS448/LpvN5vctymhdR44c0S9+8Qudd955ioiI0LBhw7Rnzx7TZXVYbrdb8+fPV//+/RUREaELL7xQixYtOuPvr6B+HTaMbNy4Uenp6crKytK+ffuUkJCglJQUHT161HRpHc57772nGTNm6K9//au2bt2q8vJyXXPNNSopKTFdWoe3e/du/dd//ZcuueQS06V0WN9//73GjRun0NBQvfXWW/rss8+0dOlSde3a1XRpHdYTTzyh1atXa8WKFdq/f7+eeOIJPfnkk1q+fLnp0tqtDvvR3qSkJI0aNUorVqyQ5P2Nnfj4eM2aNUtz5841XF3HduzYMcXGxuq9997TZZddZrqcDuvkyZO69NJLtWrVKj366KMaPny4cnJyTJfV4cydO1cffPCBduzYYboUVLrhhhsUFxenZ555xtd2yy23KCIiQv/zP/9jsLL2q0MeGSkrK9PevXuVnJzsa7Pb7UpOTtbOnTsNVgZJKi4uliR169bNcCUd24wZM3T99df7/Z2g9b3xxhtKTEzUz3/+c8XGxmrEiBFat26d6bI6tLFjxyo3N1cHDx6UJP3tb3/T+++/r/HjxxuurP0K+rdpzgXHjx+X2+32/Z5Olbi4OB04cMBQVZC8R6jmzJmjcePG8ftFBm3YsEH79u3T7t27TZfS4R06dEirV69Wenq6HnroIe3evVv33nuvwsLCNGXKFNPldUhz586Vy+XS4MGD5XA45Ha79dhjj2nSpEmmS2u3OmQYQds1Y8YMffrpp3r//fdNl9JhFRQUaPbs2dq6davCw8NNl9PheTweJSYmavHixZKkESNG6NNPP9WaNWsII4a89NJL+sMf/qD169dr6NChysvL05w5c9SrVy/WSRN1yDDSvXt3ORwOFRUV+bUXFRWpR48ehqrCzJkz9eabb2r79u264IILTJfTYe3du1dHjx7VpZde6mtzu93avn27VqxYodLSUjkcDoMVdiw9e/bURRdd5Nc2ZMgQ/fGPfzRUER544AHNnTtX//7v/y5JGjZsmL7++mtlZ2cTRpqoQ54zEhYWppEjRyo3N9fX5vF4lJubqzFjxhisrGOyLEszZ87Ua6+9pnfffVf9+/c3XVKHdtVVV+mTTz5RXl6e75KYmKhJkyYpLy+PINLKxo0bV+ej7gcPHlTfvn0NVYRTp07JbvfffTocDnk8HkMVtX8d8siIJKWnp2vKlClKTEzU6NGjlZOTo5KSEk2bNs10aR3OjBkztH79ev3pT39SZGSkCgsLJUnR0dGKiIgwXF3HExkZWed8nc6dO+u8887jPB4D7rvvPo0dO1aLFy/Wbbfdpl27dmnt2rVau3at6dI6rNTUVD322GPq06ePhg4dqo8//ljLli3TnXfeabq09svqwJYvX2716dPHCgsLs0aPHm399a9/NV1ShyQp4OW5554zXRoqXX755dbs2bNNl9Fh/e///q918cUXW06n0xo8eLC1du1a0yV1aC6Xy5o9e7bVp08fKzw83BowYID18MMPW6WlpaZLa7c67PeMAACAtqFDnjMCAADaDsIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAo/4/3VVZiWo0AmMAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#train_df, val_df = train_test_split(filtered_df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Обучалось в сумме эпох 80+-\n",
        "trained_model, history = train(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    val_dataset=val_dataset,\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    lr=1e-6,\n",
        "    device='cuda',\n",
        "    savepath='best_resnet50VBEST2.pth',\n",
        "    pre_train = 'best_resnet50VBEST.pth',\n",
        ")\n",
        "\n",
        "plt.plot(history['train_loss'], label='Train Loss')\n",
        "plt.plot(history['val_loss'], label='Val Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Статистика отфильтрованного датасета:\n",
            "{'total_images': 20000, 'unique_people': 956, 'images_per_person': {'count': 956.0, 'mean': 20.92050209205021, 'std': 1.5626391151459447, 'min': 3.0, '25%': 20.0, '50%': 21.0, '75%': 22.0, 'max': 27.0}}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Код для создания нового датасета, чтобы он включал в себя макс кол-во людей с наибольшим кол-вом картинок. Было сделано на 10к 20к и 30к\n",
        "\n",
        "\n",
        "import shutil\n",
        "\n",
        "idf_path = \"D:\\\\dlsPart2\\\\identity_CelebA.txt\"\n",
        "img_folder = \"D:\\\\dlsPart2\\\\img_align_celeba\\\\\"\n",
        "output_folder = \"D:\\\\dlsPart2\\\\img_align_celeba4\\\\\" # Новая папка для отфильтрованных изображений\n",
        "output_idf_path = \"D:\\\\dlsPart2\\\\identity_CelebA4.txt\"  # Новый файл меток\n",
        "\n",
        "# Целевой размер\n",
        "target_dataset_size = 20000\n",
        "\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "identity_df = pd.read_csv(idf_path, sep=' ', header=None, names=['filename', 'person_id'])\n",
        "files = set(os.listdir(img_folder))\n",
        "filtered_df = identity_df[identity_df['filename'].isin(files)]\n",
        "\n",
        "#сортировка по убыванию\n",
        "person_counts = filtered_df['person_id'].value_counts().sort_values(ascending=False)\n",
        "\n",
        "selected_images = []\n",
        "current_size = 0\n",
        "selected_persons = []\n",
        "\n",
        "for person_id in person_counts.index:\n",
        "    person_data = filtered_df[filtered_df['person_id'] == person_id]\n",
        "    if current_size + len(person_data) <= target_dataset_size:\n",
        "        selected_images.append(person_data)\n",
        "        current_size += len(person_data)\n",
        "        selected_persons.append(person_id)\n",
        "    else:\n",
        "        remaining = target_dataset_size - current_size\n",
        "        selected_images.append(person_data.sample(n=remaining, random_state=42))\n",
        "        selected_persons.append(person_id)\n",
        "        break\n",
        "\n",
        "filtered_df = pd.concat(selected_images, ignore_index=True)\n",
        "\n",
        "for _, row in filtered_df.iterrows():\n",
        "    src_path = os.path.join(img_folder, row['filename'])\n",
        "    dst_path = os.path.join(output_folder, row['filename'])\n",
        "    try:\n",
        "        shutil.copy(src_path, dst_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Файл не найден: {src_path}\")\n",
        "        continue\n",
        "\n",
        "# Сохранение нового файла меток\n",
        "filtered_df.to_csv(output_idf_path, sep=' ', header=False, index=False)\n",
        "\n",
        "stats = {\n",
        "    'total_images': len(filtered_df),\n",
        "    'unique_people': filtered_df['person_id'].nunique(),\n",
        "    'images_per_person': filtered_df['person_id'].value_counts().describe().to_dict()\n",
        "}\n",
        "print(\"Статистика отфильтрованного датасета:\")\n",
        "print(stats)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (myenv)",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
